{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63aefe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import time\n",
    "\n",
    "import jax\n",
    "import numpy as np\n",
    "import optax\n",
    "\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "import transformers\n",
    "\n",
    "\n",
    "from mesh_transformer import util\n",
    "from mesh_transformer.checkpoint import read_ckpt, write_ckpt\n",
    "from mesh_transformer.transformer_shard import CausalTransformer\n",
    "from tfrecord_loader import TFRecordNewInputs\n",
    "from smart_open import open\n",
    "from google.cloud import storage\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "from mesh_transformer.util import clip_by_global_norm, additive_weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "210b7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2TokenizerFast.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e314b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(network, step, bucket, path, mp, aux=None, keep_n=3, delete_old=True):\n",
    "    assert path\n",
    "    client = storage.Client()\n",
    "\n",
    "    if aux is None:\n",
    "        aux = {}\n",
    "\n",
    "    try:\n",
    "        with open(f\"gs://{bucket}/{path}/meta.json\", \"r\") as f:\n",
    "            meta = json.load(f)\n",
    "    except:\n",
    "        # create metadata file\n",
    "        with open(f\"gs://{bucket}/{path}/meta.json\", \"w\") as f:\n",
    "            json.dump({\n",
    "                \"step\": 0,\n",
    "                \"checkpoints\": [],\n",
    "                \"aux\": {}\n",
    "            }, f)\n",
    "\n",
    "    # do sharded checkpoint writing\n",
    "    start = time.time()\n",
    "    res = []\n",
    "    for shard_id in range(mp):\n",
    "        write_ckpt(network.state, f\"gs://{bucket}/{path}/step_{step}/\", shard_id)\n",
    "\n",
    "    print(f\"Wrote checkpoint in {time.time() - start:.06}s\")\n",
    "\n",
    "    with open(f\"gs://{bucket}/{path}/meta.json\", \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    meta[\"step\"] = step\n",
    "    meta[\"checkpoints\"].append(step)\n",
    "    all_aux = meta.get(\"aux\", {})\n",
    "\n",
    "    while len(meta[\"checkpoints\"]) > keep_n:\n",
    "        ckpt_to_delete = meta[\"checkpoints\"].pop(0)\n",
    "\n",
    "        try:\n",
    "            del all_aux[str(ckpt_to_delete)]\n",
    "        except:\n",
    "            print(f\"failed to delete the aux state for {step}\")\n",
    "\n",
    "        if delete_old:\n",
    "            print(f\"deleting checkpoint {ckpt_to_delete}\")\n",
    "            for blob in client.list_blobs(bucket, prefix=f\"{path}/step_{ckpt_to_delete}/\"):\n",
    "                # print(f\"deleting {blob.name}\")\n",
    "                assert path in blob.name\n",
    "                blob.delete()\n",
    "        else:\n",
    "            print(f\"keeping checkpoint {ckpt_to_delete}\")\n",
    "\n",
    "    all_aux[step] = aux\n",
    "    meta[\"aux\"] = all_aux\n",
    "\n",
    "    with open(f\"gs://{bucket}/{path}/meta.json\", \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "def eval_step(network, data):\n",
    "    inputs = {\n",
    "        \"obs\": data[:, :-1],\n",
    "        \"target\": data[:, 1:],\n",
    "    }\n",
    "\n",
    "    out = network.eval(inputs)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8773bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jax devices: 8\n",
      "jax runtime initialized in 3.42035s\n",
      "`--tune_model_path` not passed: we are continuing a fine-tuning run from a checkpoint (or we are not fine-tuning)\n",
      "state will be restored from checkpoint 383500\n",
      "path to load checkpoint from: gs://gpt-j_records_eur/records/step_383500/\n",
      "initializing network\n",
      "{'layers': 28, 'd_model': 4096, 'n_heads': 16, 'n_vocab': 50400, 'norm': 'layernorm', 'pe': 'rotary', 'pe_rotary_dims': 64, 'seq': 2048, 'cores_per_replica': 8, 'per_replica_batch': 1, 'gradient_accumulation_steps': 32, 'warmup_steps': 3000, 'anneal_steps': 27221, 'lr': 5e-05, 'end_lr': 1e-05, 'weight_decay': 0.1, 'total_steps': 30221, 'tpu_size': 8, 'bucket': 'gpt-j_records_eur', 'model_dir': 'models_cp_crypto_subset', 'train_set': 'crypto_subset.train.index', 'val_set': {'crypto_scratch': 'crypto_subset.val.index'}, 'eval_harness_tasks': [], 'val_batches': 100, 'val_every': 100, 'ckpt_every': 1000, 'keep_every': 5000, 'name': 'GPT3_crypto_subset', 'comment': '', 'optimizer': GradientTransformation(init=<function scale.<locals>.init_fn at 0x7f8106316c10>, update=<function scale.<locals>.update_fn at 0x7f82956d0af0>)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/Mcian/.local/lib/python3.8/site-packages/jax/experimental/maps.py:412: UserWarning: xmap is an experimental feature and probably has bugs!\n",
      "  warn(\"xmap is an experimental feature and probably has bugs!\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key shape (8, 2)\n",
      "in shape (1, 2048)\n",
      "dp 1\n",
      "mp 8\n",
      "Total parameters: 6053381344\n",
      "loading network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 851, in next\n",
      "    item = self._items.popleft()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_202421/1881186042.py\", line 83, in <module>\n",
      "    network.state = read_ckpt(network.state, initial_ckpt_state_path, devices.shape[1], load_opt=False)\n",
      "  File \"/home/Mcian/mesh-transformer-jax/mesh_transformer/checkpoint.py\", line 135, in read_ckpt\n",
      "    shards = list((p.imap(read_shard, [f\"{dir}shard_{i}/\" for i in range(shards_in)])))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 856, in next\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 302, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 851, in next\n",
      "    item = self._items.popleft()\n",
      "IndexError: pop from an empty deque\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_202421/1881186042.py\", line 83, in <module>\n",
      "    network.state = read_ckpt(network.state, initial_ckpt_state_path, devices.shape[1], load_opt=False)\n",
      "  File \"/home/Mcian/mesh-transformer-jax/mesh_transformer/checkpoint.py\", line 135, in read_ckpt\n",
      "    shards = list((p.imap(read_shard, [f\"{dir}shard_{i}/\" for i in range(shards_in)])))\n",
      "  File \"/usr/lib/python3.8/multiprocessing/pool.py\", line 856, in next\n",
      "    self._cond.wait(timeout)\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 302, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3361, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3458, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2063, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(etype,\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/Mcian/.local/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/usr/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/usr/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = 'configs/crypto_subset_8.json'\n",
    "    fine_tuning = False\n",
    "    params = json.load(open(config))\n",
    "\n",
    "    gradient_accumulation_steps = params.get(\"gradient_accumulation_steps\", 1)\n",
    "    per_replica_batch = params[\"per_replica_batch\"]\n",
    "    cores_per_replica = params[\"cores_per_replica\"]\n",
    "\n",
    "    assert cores_per_replica <= 8\n",
    "\n",
    "    bucket = params[\"bucket\"]\n",
    "    model_dir = params[\"model_dir\"]\n",
    "    layers = params[\"layers\"]\n",
    "    d_model = params[\"d_model\"]\n",
    "    n_heads = params[\"n_heads\"]\n",
    "    n_vocab = params[\"n_vocab\"]\n",
    "    seq = params[\"seq\"]\n",
    "    norm = params[\"norm\"]\n",
    "\n",
    "    val_batches = params[\"val_batches\"]\n",
    "    val_every = params[\"val_every\"]\n",
    "    ckpt_every = params[\"ckpt_every\"]\n",
    "    keep_every = params[\"keep_every\"]\n",
    "    eval_tasks = params[\"eval_harness_tasks\"]\n",
    "    total_steps = params[\"total_steps\"]\n",
    "\n",
    "    pe = params[\"pe\"]\n",
    "    assert pe in [\"fixed\", \"rotary\", \"t5\"]\n",
    "\n",
    "    warmup_steps = params[\"warmup_steps\"]\n",
    "    anneal_steps = params[\"anneal_steps\"]\n",
    "    lr = params[\"lr\"]\n",
    "    end_lr = params[\"end_lr\"]\n",
    "    weight_decay = params[\"weight_decay\"]\n",
    "\n",
    "    opt = optax.scale(0)\n",
    "\n",
    "    params[\"optimizer\"] = opt\n",
    "\n",
    "    start = time.time()\n",
    "    tpu_size = jax.device_count()\n",
    "    if tpu_size < cores_per_replica:\n",
    "        msg = f\"each shard needs a separate device, but device count ({tpu_size}) < shard count ({cores_per_replica})\"\n",
    "        raise ValueError(msg)\n",
    "    print(f\"jax devices: {tpu_size}\")\n",
    "    print(f\"jax runtime initialized in {time.time() - start:.06}s\")\n",
    "\n",
    "    mesh_shape = (tpu_size // cores_per_replica, cores_per_replica)\n",
    "    devices = np.array(jax.devices()).reshape(mesh_shape)\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    print('`--tune_model_path` not passed: we are continuing a fine-tuning run from a checkpoint (or we are not fine-tuning)')\n",
    "#     initial_ckpt_path = f\"gs://gpt-j_records_eur/models_cp_crypto_subset\"\n",
    "    initial_ckpt_path = f\"gs://gpt-j_records_eur/records\"\n",
    "#     meta_path = f\"{initial_ckpt_path}/meta.json\"\n",
    "\n",
    "#     with open(meta_path, \"r\") as f:\n",
    "#         meta = json.load(f)\n",
    "#     ckpt_step = meta[\"checkpoints\"][-1]\n",
    "    ckpt_step = 383500\n",
    "    initial_ckpt_state_path = f\"{initial_ckpt_path}/step_{ckpt_step}/\"\n",
    "    print(f\"state will be restored from checkpoint {ckpt_step}\")\n",
    "\n",
    "    step = ckpt_step\n",
    "\n",
    "    if initial_ckpt_state_path:\n",
    "        print(f\"path to load checkpoint from: {initial_ckpt_state_path}\")\n",
    "    else:\n",
    "        print(\"not loading from a checkpoint\")\n",
    "\n",
    "    # load + run\n",
    "    with jax.experimental.maps.mesh(devices, ('dp', 'mp')):\n",
    "        print(\"initializing network\")\n",
    "        print(params)\n",
    "        network = CausalTransformer(params)\n",
    "\n",
    "        if initial_ckpt_state_path:\n",
    "            print(\"loading network\")\n",
    "\n",
    "            start = time.time()\n",
    "            network.state = read_ckpt(network.state, initial_ckpt_state_path, devices.shape[1], load_opt=False)\n",
    "\n",
    "            print(f\"network loaded in {time.time() - start:.06}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4899393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(context, top_p=0.9, temp=1.0, gen_len=512):\n",
    "    length = np.ones(context.shape[0], dtype=np.uint32) * context.shape[1]\n",
    "    output = network.generate(context, length, gen_len, \n",
    "                              {\"top_p\": np.ones(total_batch) * top_p, \"temp\": np.ones(total_batch) * temp}, return_logits=True)\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_p = 0.9 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "temp = 1 #@param {type:\"slider\", min:0, max:1, step:0.1}\n",
    "\n",
    "end_token = 26\n",
    "\n",
    "val_files = [f for f in listdir('strings/val/')]\n",
    "\n",
    "results = {}\n",
    "for f in val_files:\n",
    "    if f != 'BTC_1min_string.txt':\n",
    "        continue\n",
    "    print(f)\n",
    "        \n",
    "    words = Path(f'strings/val/{f}').read_text()\n",
    "    tokens = tokenizer.encode(words)\n",
    "    i = 2048\n",
    "    all_contexts = np.array([])\n",
    "    outs = np.array([])\n",
    "    while i < len(tokens):\n",
    "        contexts = []\n",
    "        while len(contexts) < 100:\n",
    "            print(i)\n",
    "            context = tokens[:i]\n",
    "            while context[-1] != end_token:\n",
    "                i += 1\n",
    "                context = tokens[i-2048:i]\n",
    "            contexts.append(context)\n",
    "            \n",
    "        contexts = np.array(contexts)\n",
    "        all_contexts = np.concat([all_contexts, contexts])\n",
    "        i += 1\n",
    "        outs = np.concat([outs, infer(top_p=top_p, temp=temp, gen_len=100, context=contexts)])\n",
    "    results[f] = {'outs': outs, 'contexts': all_contexts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c5d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trues = []\n",
    "preds = []\n",
    "for f, v in results.items():\n",
    "    print(f)\n",
    "    contexts = v['contexts']\n",
    "    targets = []\n",
    "    for context in contexts:\n",
    "        context = list(context)\n",
    "        target = context[:context.index(end_token)+1]\n",
    "        targets.append(target)\n",
    "    \n",
    "    for context, out, target in zip(contexts[:-1], outs[:-1], targets[1:]):\n",
    "        out = out[:context.index(end_token)]\n",
    "        out = tokenizer.decode(out[1][0])\n",
    "        target = tokenizer.decode(target)\n",
    "        \n",
    "        target_values = re.findall('\\d+\\_\\d+', target) + re.findall('\\d+\\:\\d+', target) + re.findall('\\d+\\.\\d+', target)\n",
    "        out_values = re.findall('\\d+\\_\\d+', out) + re.findall('\\d+\\:\\d+', out) + re.findall('\\d+\\.\\d+', out)\n",
    "        if target_values[0] == out_values[0] and target_values[1] == out_values[1]:\n",
    "            trues.append(target_values[2:])\n",
    "            preds.append(out_values[2:])\n",
    "        else:\n",
    "            print('skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbc39bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
